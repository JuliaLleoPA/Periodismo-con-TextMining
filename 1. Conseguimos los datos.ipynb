{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:orange\"> WEB SCRAPING </span>\n",
    "___\n",
    "La función de *web scraping* es una técnica de recuperación de información web. La recuperación automática sigue el protocolo habitual de acceso a cualquier web, y se realiza a través de una petición al servidor del contenido de una URL siguiendo el **protocolo HTTP**.\n",
    "Lo que vamos a hacer con Python es automatizar esta petición y recibir el código fuente de nuestra página y, gracias a las funciones de la librería **BeautifulSoup**, analizar el etiquetado HTML para extraer las partes que nos interesan. Como se muestra en la función, es necesario haber inspeccionado con anterioridad la página que se quiere rastrear, y haber identificado la etiqueta *html* que nos interesa recoger (en el ejemplo de función más abajo, se buscan los párrafos: `<p>`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos nuestras dos librerías previamente instaladas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función general para hacer web scraping\n",
    "\n",
    "def url_periodico(url):\n",
    "\n",
    "    '''Recupera el texto de la URL facilitada por una petición HTTP'''\n",
    "    pagina = requests.get(url).text\n",
    "    \n",
    "    '''BeautifulSoup acepta dos argumentos: el marcado actual, y el parser que se quiere usar.\n",
    "    En nuestro caso escogemos el parser \"lxml\", ya que también funciona para versiones antiguas de Python y es muy rápido. \n",
    "    Para ello, es preciso instalarlo antes en nuestro equipo'''\n",
    "    soup = BeautifulSoup(pagina, \"lxml\")\n",
    "    \n",
    "    '''Esta operación recupera el texto dentro de las etiquetas <p> de clase \"article-text\" (recordemos que la clase se hereda)'''\n",
    "    text = [p.text for p in soup.find_all('p')]\n",
    "    \n",
    "    print(url , \"recuperado\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez aplicamos la función a la lista de URLs que nos interesan, almacenamos el resultado de tipo cadena de texto como una lista.\n",
    "\n",
    "`periodico = [variable_primerPeriodico, variable_segundoPeriodico, ... ]`\n",
    "        \n",
    "Deberemos valorar si para la obtención del contenido que nos interesa es preciso definir nuevas funciones (por ejemplo, si no solo nos interesa recoger los párrafos `<p`, sino también los encabezados `<h1>`).\n",
    "\n",
    "Para poder guardar en distintos archivos de texto el material recogido, crearemos una nueva lista con los nombres que queremos para cada archivo.\n",
    "\n",
    "`nombrePeriodicos = ['Nombre del primer periódico', 'Nombre del segundo periódico', ... ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:orange\"> ARCHIVOS PICKLE </span>\n",
    "___\n",
    "\n",
    "Para almacenar este raspado utilizamos el módulo Pickle de Python, que nos permite representar nuestros objetos como cadenas de bytes sin necesidad de realizar ninguna conversión especial. Estos archivos pickles nos permitirán, gracias a su serialización, almacenar los resultados para próximos programas Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el módulo previamente instalado\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos los archivos\n",
    "\n",
    "'''Fíjate en que a cada variable dentro de la lista \"periodico\" le corresponda su nombre'''\n",
    "\n",
    "for i, c in enumerate(nombrePeriodicos):\n",
    "    \n",
    "    '''Previamente hemos creado una carpeta llamada \"corpus\" donde guardaremos nuestros objetos serializados'''\n",
    "    '''El indicador wb nos sirve para crear el archivo de escritura en modo binario'''\n",
    "\n",
    "    with open(\"corpus/\" + c + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(periodico[i], file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
